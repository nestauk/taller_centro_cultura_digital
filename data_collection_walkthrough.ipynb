{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Scraping web data\n",
    "\n",
    "In this analysis we will scrape actor biographies from IMDB, which can be used to build a recommendation engine in part 2. This notebook will guide you through the process. See [here](https://github.com/nestauk/taller_centro_cultura_digital/blob/master/data_collection.ipynb) for one possible solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from retrying import retry\n",
    "import json\n",
    "\n",
    "# The first page we're scraping\n",
    "TOP_PAGE = \"https://www.imdb.com/list/ls058011111/\"\n",
    "\n",
    "# The second page we're scraping\n",
    "BIO_PAGE = \"https://imdb.com/name/{}/bio?ref_=nmls_hd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the scene\n",
    "\n",
    "We would like to scrape each actor's biography (for example [Robert De Niro](https://www.imdb.com/name/nm0000134/bio?ref_=nm_ql_1)'s), by iterating through the [list indicated here](https://www.imdb.com/list/ls058011111/).\n",
    "\n",
    "#### a) Look at the source code (HTML) for the list. You can do that by either right-clicking on the page and clicking \"show source\". Can you see any patterns in the HTML which you could use to infer the URL for Robert De Niro's biography. (Hint: what is special about https://www.imdb.com/name/nm0000134/bio?ref_=nm_ql_1 compared to the biography for another actor? Can you find this in the list's source code?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the general gist of web scraping: finding patterns you can exploit. We can download the source code (HTML) of a web page using python `requests`. Get the source code by executing the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(TOP_PAGE)\n",
    "r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then inspect the HTML directly, but it looks horrible because python treats the HTML as plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better idea is to use `BeautifulSoup`, which makes it very easy to work with HTML (and makes it look pretty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b i) The following example shows how you can find every image on the page, as each image in HTML is denoted by the tag \"`img`\". Feel free to execute the code. Now change the code, so that instead of scraping `img` tags, you scrape the tags associated with URLs. (Hint: if you right-click and inspect any URL, your browser will indicate what tag you need)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for element in soup.find_all(\"img\"):\n",
    "    print(element)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b ii) In the following code sample, we can inspect the properties of the `img` tag. In the below example, we can extract the `width` property of each image. Feel free to execute the code. Now modify the code so that you extract the URL property from tag you identified above, and store them in a list called 'urls'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "widths = []\n",
    "for element in soup.find_all(\"img\"):\n",
    "    if 'width' not in element.attrs:\n",
    "        continue\n",
    "    widths.append(element['width'])\n",
    "print(widths)\n",
    "\n",
    "# urls = []\n",
    "# for element in soup.find_all(???):\n",
    "#     if ??? not in element.attrs:\n",
    "#         continue\n",
    "#     urls.append(element[???])\n",
    "# print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b iii) Now that we have the URLs, we actually have too many! Filter the URLs by filling in the following code.\n",
    "\n",
    "#### b iv) Let's extract the codes directly from the URLs. First, uncomment all *lines* which start with a '#'. Next, extract the codes from the URLs using the range method indicated below (for example `url[3:10]` would give all characters from the 4th and 10th)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codes = []\n",
    "for url in urls:\n",
    "    if not url.startswith(???):  #What do the URLs that we need start with?\n",
    "        continue\n",
    "    if not url.endswith(???):  # What do the URLs that we need end with?\n",
    "        continue\n",
    "    #code = url[???:???]  # What numeric range ([start:end]) is required to extract the actor's code?\n",
    "    #codes.append(code)\n",
    "#print(codes)\n",
    "#print(\"Found\", len(codes), \"codes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b v) Let's wrap this up in a function, because we going to need to use it again later. Check that the function works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop_max_attempt_number=7, wait_fixed=2000)  # Ask a helper why this is needed!\n",
    "def get_codes_from_top_page(tag_name, property_name, url_start, url_end):\n",
    "    r = requests.get(TOP_PAGE)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Now extract codes from the soup\n",
    "    codes = []\n",
    "    for element in soup.find_all(tag_name):\n",
    "        if property_name not in element.attrs:\n",
    "            continue\n",
    "        url = element[property_name]\n",
    "        if not url.startswith(url_start):  #What do the URLs that we need start with?\n",
    "            continue\n",
    "        if not url.endswith(url_end):  # What do the URLs that we need end with?\n",
    "            continue\n",
    "        code = url[len(url_start):-len(url_end)] \n",
    "        codes.append(code)    \n",
    "    return codes\n",
    "\n",
    "# Check the function works\n",
    "#codes = get_codes_from_top_page(tag_name=???, property_name=???, url_start=???, url_end=???)\n",
    "print(codes)\n",
    "print(\"Found\", len(codes), \"codes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrolling through pages\n",
    "\n",
    "Great, so now we should have 100 codes... but wait - doesn't the list say that it has the 'Top 1000 Actors and Actresses'? [Looking back at the list](https://www.imdb.com/list/ls058011111/), click on the 'NEXT' link at the bottom of the page. We can see that the list has been updated to include the next 100 actors.\n",
    "\n",
    "The URL for the page has also changed, slightly. The main URL (before '?') remains the same, however the parameters (everything after '?') have been added. The only one we care about is the 'page' parameter. Which can be inputted as so:\n",
    "\n",
    "    r = requests.get(TOP_PAGE, params={\"page\": 2})\n",
    "\n",
    "#### c) First, change the function `get_codes_from_top_page` to include the page number in the request. Then extract the first 3 pages by executing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_pages = 3\n",
    "codes = []\n",
    "for page in range(1, n_pages+1):\n",
    "    codes += get_codes_from_top_page(tag_name=???, property_name=???, url_start=???, url_end=???, page=page)\n",
    "    \n",
    "# Remove duplicates\n",
    "codes = set(codes)\n",
    "print(codes)\n",
    "print(\"Found\", len(codes), \"codes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting biographies\n",
    "\n",
    "Now we're able to extract all 1000 codes. The next step (and the point of this walkthrough!) is to extract the biographies for each actor. At the top of this notebook, you'll see that a variable `BIO_PAGE` has been defined as:\n",
    "\n",
    "    BIO_PAGE = \"https://imdb.com/name/{}/bio?ref_=nmls_hd\"\n",
    "    \n",
    "The '{}' is python syntax for string formatting. Play with the following code until you understand how '{}' works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"joel\"\n",
    "other_name = \"juan\"\n",
    "\n",
    "text = \"My name is {}\"\n",
    "other_text = \"Our names are {} and {}\"\n",
    "\n",
    "print(text)\n",
    "print(text.format(name))\n",
    "print(other_text.format(name, other_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d i) Iterate through `codes` and generate all biography urls. Copy and paste a few into your browser to confirm that they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for code in codes:\n",
    "    bio_url = ???\n",
    "    print(bio_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d ii) Take Robert De Niro's biography url (https://www.imdb.com/name/nm0000134/bio?ref_=nm_ql_1), and inspect the page in your browser. Find the 'div' which contains the biography text, and input the class names below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_url = \"https://www.imdb.com/name/nm0000134/bio?ref_=nm_ql_1\"\n",
    "r = requests.get(bio_url)\n",
    "r.raise_for_status()\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "element = soup.find(\"div\", class_=[???,???])  # Enter the two class names here. Ask a helper why \"class\" ends with \"_\"!\n",
    "bio_text = element.text\n",
    "print(bio_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so you've collected the data, but the biography isn't very useful unless we have the actor's name. \n",
    "\n",
    "#### d iv) Extract the Robert De Niro's name from the soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "element = soup.find(???, property=???)\n",
    "actor_name = element[???]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together\n",
    "\n",
    "Let's put all of section d) into a single function.\n",
    "\n",
    "#### e i) Finish writing the code for the following function. Check that it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_bio_and_name(imdb_code):\n",
    "    bio_url = BIO_URL.format(???)\n",
    "    r = requests.get(bio_url)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # Get the bio text\n",
    "    element = soup.find(\"div\", class_=[???,???])  # Enter the two class names here. Ask a helper why \"class\" ends with \"_\"!\n",
    "    bio_text = element.text\n",
    "    \n",
    "    # Get the actor's name\n",
    "    element = soup.find(???, property=???)\n",
    "    actor_name = element[???]\n",
    "\n",
    "    return actor_name, bio_text\n",
    "    \n",
    "actor_name, bio_text = extract_bio_and_name(\"nm0000134\")\n",
    "print(\"--->\", actor_name)\n",
    "print(\"===>\", bio_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e ii) Now iterate through all of your codes and generate the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for code in codes:\n",
    "    actor_name, bio_text = extract_bio_and_name(???)\n",
    "    data[actor_name] = bio_text\n",
    "    \n",
    "print(\"Got\", len(data), \"biographies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e iii) Finally, save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fetch each biography for every code.\n",
    "with open(\"data/out-bios.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras\n",
    "\n",
    "Well done, you've finished your first web scraping. I hope you can see that you could apply this to many data sources that could be of interest to you. There are a couple of extra problems that you could still solve however:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f i) Most of the biographies contain some text similar to _'- IMDb Mini Biography By:  Pedro Borges'_. Add some code into your function to remove this.\n",
    "\n",
    "#### f ii) This is only 1000 actors, but there are many more on IMDB. Think of a strategy for collecting more! (Hint: there are many ways to do this, one way might be to look for links on the biography pages)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
