{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building a recommendation system\n",
    "\n",
    "In this analysis we will build a recommendation engine based on the IMDB actor biographies extracted in part 1. This notebook will guide you through the process. See [here](https://github.com/nestauk/taller_centro_cultura_digital/blob/master/data_analysis.ipynb) for one possible solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB: replace '???' with your own code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text processing\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from joel_tools import SynonymBuilder\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For general data manipulations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For importing the biography data\n",
    "import json\n",
    "\n",
    "# Load this in advance - this will parse our text for us\n",
    "PARSER = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "First we'll get started by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the data from Part 1\n",
    "with open(\"data/bios.json\") as f:\n",
    "    bios = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)  Take a look at the data and understand it's structure. How many \"rows\" are there? How much variation is there in the length of the biographies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting tokens\n",
    "\n",
    "In data science terms, this isn't a lot of data, but it's enough to work with if we use some tricks. First we need to split up each biography into 'tokens' (these could be individual words, or a number of consective words).\n",
    "\n",
    "#### b i) Run the PARSER on the \"Robert De Niro's\" biography and inspect the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bio = ???  # Replace ??? with \"Robert De Niro's\" biography\n",
    "tokens = PARSER(bio)\n",
    "for t in tokens:\n",
    "    print(t.text, \"\\t\", t.is_alpha, t.is_punct, t.is_stop, t.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the PARSER has automatically identified 'entities' such as names and places. It would be best if these were treated as a single token. We can do that by merging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge entities, and take a note of them\n",
    "entities = set()\n",
    "for entity in tokens.ents:\n",
    "    entity.merge()\n",
    "    entities.add(entity.text)\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b ii) Do the entities make sense? Which types of entities will not be useful when generating a recommendation engine? Remember: our aim here is to reduce the size of the vocubulary by removing tokens which could introduce strange biases into our recommendation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_entities = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate a smaller list of tokens for \"Robert De Niro\" by excluding these bad entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [t for t in tokens\n",
    "          if (t.is_alpha or t.text in entities)\n",
    "          and not (t.is_punct or t.is_stop\n",
    "                   or t.ent_type_ in bad_entities)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b iii) Inspect the `t.lemma_` attribute for each token `t`. How is the `lemma_` related to the token? Why will it be useful for us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in tokens:\n",
    "    print(t, ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the above steps into a single 'tokenizer' function, which generate our tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence, bad_entities):\n",
    "    \"\"\"Split a sentence into tokens\"\"\"\n",
    "    # Initial parsing\n",
    "    tokens = PARSER(sentence)\n",
    "    \n",
    "    # Merge entities, and take a note of them\n",
    "    entities = set()\n",
    "    for entity in tokens.ents:\n",
    "        entity.merge()\n",
    "        entities.add(entity.text)\n",
    "        \n",
    "    # Only accept tokens which are made of letters, and are not\n",
    "    # organisations, people or dates\n",
    "    tokens = [t for t in tokens\n",
    "              if (t.is_alpha or t.text in entities)\n",
    "              and not (t.is_punct or t.is_stop\n",
    "                       or t.ent_type_ in bad_entities)]\n",
    "    \n",
    "    # Lowercase and strip the text of excess spaces\n",
    "    tokens = [t.lemma_.lower().strip()\n",
    "              if (t.lemma_ != \"-PRON-\" and\n",
    "                  t.text not in entities)\n",
    "              else t.lower_ for t in tokens]\n",
    "\n",
    "    # Finally, entities starting with pronouns are overkill for this analysis\n",
    "    # so strip off the pronouns to maximise term counts\n",
    "    for start in [\"a\", \"an\", \"the\"]:\n",
    "        n = len(start) + 1\n",
    "        tokens = [t[n:] if t.startswith(f\"{start} \")\n",
    "                  else t for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b iv) Confirm that the `spacy_tokenizer` runs as expected on 10 biographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, name in enumerate(bios):\n",
    "    if i == 10:\n",
    "        break\n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the vocabulary\n",
    "\n",
    "If we inspect the entire vocabulary, we might find some more patterns which we can exploit to reduce the size of our vocabulary.\n",
    "\n",
    "####  c i) Create a list of every token from the first 100 biographies, and look at the unique terms with `set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for i, name in enumerate(bios):\n",
    "    if i == 100:\n",
    "        break\n",
    "    ???\n",
    "\n",
    "# Get the unique terms\n",
    "vocab_set = set(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c ii) In the vocabulary there are some tokens that we might want to consider as being the same. For example, inspect every token in `vocab_set` which contains the substring \"golden globe\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c iii) Discuss with a nearby participant a strategy for combining these terms together. Feel free to look at [SynonymBuilder](https://github.com/nestauk/taller_centro_cultura_digital/blob/master/joel_tools/synonym_builder.py) for inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c iv) Apply `SynonynBuilder` to your vocabulary, and confirm that it has reduced the size of the vocabulary as expected. No algorithm is perfect - what features of `SynonynBuilder` might you like to improve, looking at the results (for example, search for \"golden globe\" again)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn_builder = SynonymBuilder()\n",
    "reduced_vocab = syn_builder.fit_transform(vocab)\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c v) The PARSER appears to have done a bad job with some names (again, no algorithm is perfect!), and second names such as 'smith' still appear in the vocab. In the context of a recommendation engine, suggest three kinds of bias that could this introduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.\n",
    "    2.\n",
    "    3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c vi) Remove all first names and second names from your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in bios.keys():\n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the tokenizer, we can wrap up all of this code into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_vocabulary(names, texts, bad_entities):\n",
    "    \"\"\"Generate the vocabulary to be used in the analysis. In order to maximise our\n",
    "    chance of getting reasonable results, we need to increase our token counts. We therefore\n",
    "    use the SynonymBuilder to decide which terms are actually the same.\"\"\" \n",
    "    # Build the basic vocab from the tokenizer\n",
    "    vocab = []\n",
    "    for text in texts:\n",
    "        vocab += spacy_tokenizer(text, bad_entities)\n",
    "        \n",
    "    # Use the synonym builder to reduce the data size\n",
    "    syn_builder = SynonymBuilder()\n",
    "    vocab = syn_builder.fit_transform(vocab)\n",
    "    texts = syn_builder.transform(texts)\n",
    "    \n",
    "    # Remove the author's name from the vocabulary\n",
    "    vocab = set(vocab)\n",
    "    for name in names:\n",
    "        vocab = vocab - set(name.lower().split())\n",
    "    return vocab, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c vii) Apply `clean_vocabulary` to the first 50 biographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ???\n",
    "texts = ???\n",
    "vocab, texts = clean_vocabulary(names, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing: turning text into data\n",
    "\n",
    "We convert text into data by \"vectorizing\". The most simple way to vectorize would be to apply a simple 'binary' vectorizer. The default tokenizer will be used, and no 'cleaning' of the vocabulary is performed. \n",
    "\n",
    "#### d i) Apply a binary `CountVectorizer` to the first 50 biographies. What do you think of the vocabulary (column names)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "data = cv.fit_transform(texts)\n",
    "df_binary = pd.DataFrame(data.todense(), columns=cv.get_feature_names(), index=names)\n",
    "df_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, of course, use our own tokenizer and cleaned vocabulary. \n",
    "\n",
    "#### d ii) Apply a binary `CountVectorizer` using the `spacy_tokenizer` cleaned vocabulary. How do you feel about the vocabulary? Are there still junk terms in the vocabulary? Could you modify `spacy_tokenizer` in order to give a slightly better result (for example replacing '\"' quotations?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_entities = ???\n",
    "cv = CountVectorizer(binary=True, vocabulary=vocab, tokenizer=lambda x: spacy_tokenizer(x, bad_entities))\n",
    "data = cv.fit_transform(texts)\n",
    "df_binary = pd.DataFrame(data.todense(), columns=cv.get_feature_names(), index=names)\n",
    "df_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d iii) Discuss with a nearby participant why using a binary count could be both good and bad. (Hint: think in terms of the importance of each token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d iv) Generally a better strategy is to use the `TfidfVectorizer` instead of a binary count. Apply the `TfidfVectorizer`, and describe what low and high values of 'tfidf' physically mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_entities = ???\n",
    "tv = TfidfVectorizer(vocabulary=vocab, tokenizer=lambda x: spacy_tokenizer(x, bad_entities))\n",
    "data = tv.fit_transform(texts)\n",
    "df_tfidf = pd.DataFrame(data.todense(), columns=tv.get_feature_names(), index=names)\n",
    "\n",
    "# Get just the row for Robert De Niro\n",
    "_df = df_tfidf.loc[(df_tfidf.index == 'Robert De Niro')]\n",
    "_df = _df[_df != 0.0].dropna(axis=1)\n",
    "\n",
    "# Sort by tfidf\n",
    "_df.T.sort_values('Robert De Niro', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d v) Now look at the 'shape' of your data (i.e. print `data.shape`). What do the two numbers correspond to? Soon we're going to try to find similar rows in our data. Why might the current shape our data make that difficult?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The recommendation engine\n",
    "\n",
    "A common strategy for reducing your data size is using Principal Component Analysis ('PCA'). This will compact the number of columns before we examine the similarity of rows. We'll apply the 'cosine similarity' and examine the most similar actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduce to 75% of original size\n",
    "pca = PCA(0.75)\n",
    "_data = pca.fit_transform(data.todense())\n",
    "\n",
    "# Calculate similarity \n",
    "sims = cosine_similarity(_data) - np.eye(data.shape[0])\n",
    "\n",
    "# Build a dataframe to display the results of the recommendation engine\n",
    "most_similar = []\n",
    "for name, row in zip(names, sims):\n",
    "    highest = row.max()\n",
    "    found = False\n",
    "    for _name, score in zip(names, row):\n",
    "        if np.isclose(score, highest):\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        continue\n",
    "    most_similar.append(dict(name=name, most_similar=_name, score=score))\n",
    "    \n",
    "df_sim = pd.DataFrame(most_similar, columns=[\"name\",\"most_similar\",\"score\"]).sort_values(\"score\", ascending=False)\n",
    "df_sim.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e i) Play with the PCA value (between 0 and 1), and describe the effect on the recommendation engine's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e ii) Try using `euclidean_distances` instead of `cosine_similarity`, what is the effect? How can you explain this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    <Put your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e iii) Run the recommendation system on the full dataset. Compare results with your fellow participants. Try to understand how you may have got different results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
