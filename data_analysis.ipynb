{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building a recommendation system\n",
    "\n",
    "In this analysis we will build a recommendation engine based on the IMDB actor biographies extracted in part 1. This notebook shows one possible solution to part 2 of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joel_tools import SynonymBuilder\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For general data manipulations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For importing the biography data\n",
    "import json\n",
    "\n",
    "# Load this in advance - this will parse our text for us\n",
    "PARSER = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a good vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    \"\"\"Split a sentence into tokens\"\"\"\n",
    "    # Initial parsing\n",
    "    tokens = PARSER(sentence)\n",
    "    \n",
    "    # Merge entities, and take a note of them\n",
    "    entities = set(\n",
    "    for entity in tokens.ents:\n",
    "        entity.merge()\n",
    "        entities.add(entity.text)\n",
    "        \n",
    "    # Only accept tokens which are made of letters, and are not\n",
    "    # organisations, people or dates\n",
    "    tokens = [t for t in tokens\n",
    "              if (t.is_alpha or t.text in entities)\n",
    "              and not (t.is_punct or t.is_stop\n",
    "                       or t.ent_type_ in [\"PERSON\", \"GPE\", \"DATE\"])]\n",
    "    \n",
    "    # Lowercase and strip the text of excess spaces\n",
    "    tokens = [t.lemma_.lower().strip()\n",
    "              if (t.lemma_ != \"-PRON-\" and\n",
    "                  t.text not in entities)\n",
    "              else t.lower_ for t in tokens]\n",
    "\n",
    "    # Finally, entities starting with pronouns are overkill for this analysis\n",
    "    # so strip off the pronouns to maximise term counts\n",
    "    for start in [\"a\", \"an\", \"the\"]:\n",
    "        n = len(start) + 1\n",
    "        tokens = [t[n:] if t.startswith(f\"{start} \")\n",
    "                  else t for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def clean_vocabulary(bios):\n",
    "    \"\"\"Generate the vocabulary to be used in the analysis. In order to maximise our\n",
    "    chance of getting reasonable results, we need to increase our token counts. We therefore\n",
    "    use the SynonymBuilder to decide which terms are actually the same.\"\"\"\n",
    "    names = bios.keys()\n",
    "    texts = bios.values()\n",
    " \n",
    "    # Build the basic vocab from the tokenizer\n",
    "    vocab = []\n",
    "    for text in texts:\n",
    "        vocab += spacy_tokenizer(text)\n",
    "        \n",
    "    # Use the synonym builder to reduce the data size\n",
    "    syn_builder = SynonymBuilder()\n",
    "    vocab = syn_builder.fit_transform(vocab)\n",
    "    texts = syn_builder.transform(texts)\n",
    "    \n",
    "    # Remove the author's name from the vocabulary\n",
    "    vocab = set(vocab)\n",
    "    for name in names:\n",
    "        vocab = vocab - set(name.lower().split())\n",
    "    return vocab, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"main\" code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the data from Part 1\n",
    "with open(\"data/bios.json\") as f:\n",
    "    bios = json.load(f)          \n",
    "\n",
    "# Generate a clean vocabulary and convert the text to data\n",
    "clean_vocab, texts = clean_vocabulary(bios)\n",
    "cv = TfidfVectorizer(tokenizer=spacy_tokenizer,\n",
    "                     vocabulary=clean_vocab, min_df=5, max_df=0.95)                                   \n",
    "data = cv.fit_transform(texts)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many more columns of data than we have rows! There is no way this will give a good result (it's good to think why that is). We therefore reduce the data size using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduce to 75% of original size\n",
    "pca = PCA(0.75)\n",
    "_data = pca.fit_transform(data.todense())\n",
    "_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better, now we can build the similarity matrix, and extract the most similar actor for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims = cosine_similarity(_data)\n",
    "# Don't compare actors to themselves!\n",
    "sims = sims - np.eye(sims.shape[0])\n",
    "\n",
    "most_similar = []\n",
    "for name, row in zip(bios.keys(), sims):\n",
    "    highest = row.max()\n",
    "    found = False\n",
    "    for _name, score in zip(bios.keys(), row):\n",
    "        if np.isclose(score, highest):\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        continue\n",
    "    most_similar.append(dict(name=name, most_similar=_name, score=score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the data into a pandas DataFrame will make read the data much easier. What do you think of the results? Pretty good I'd say! Now imagine applying this routine to any data you have. Could you think of practical uses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>most_similar</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>Johnny Knoxville</td>\n",
       "      <td>Bonnie Hunt</td>\n",
       "      <td>0.789569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>Bonnie Hunt</td>\n",
       "      <td>Johnny Knoxville</td>\n",
       "      <td>0.789569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Carrie Fisher</td>\n",
       "      <td>Mark Hamill</td>\n",
       "      <td>0.777845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>Mark Hamill</td>\n",
       "      <td>Carrie Fisher</td>\n",
       "      <td>0.777845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>Martin Lawrence</td>\n",
       "      <td>Johnny Knoxville</td>\n",
       "      <td>0.776004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>Craig Robinson</td>\n",
       "      <td>Danny McBride</td>\n",
       "      <td>0.729525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>Danny McBride</td>\n",
       "      <td>Craig Robinson</td>\n",
       "      <td>0.729525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Emma Watson</td>\n",
       "      <td>Daniel Radcliffe</td>\n",
       "      <td>0.710853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Daniel Radcliffe</td>\n",
       "      <td>Emma Watson</td>\n",
       "      <td>0.710853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>William Shatner</td>\n",
       "      <td>Patrick Stewart</td>\n",
       "      <td>0.685567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>Patrick Stewart</td>\n",
       "      <td>William Shatner</td>\n",
       "      <td>0.685567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>Bill Hader</td>\n",
       "      <td>Bonnie Hunt</td>\n",
       "      <td>0.671170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>Mindy Kaling</td>\n",
       "      <td>Bill Hader</td>\n",
       "      <td>0.667186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>Jean-Claude Van Damme</td>\n",
       "      <td>Chuck Norris</td>\n",
       "      <td>0.663727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>Chuck Norris</td>\n",
       "      <td>Jean-Claude Van Damme</td>\n",
       "      <td>0.663727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>June Squibb</td>\n",
       "      <td>Bonnie Hunt</td>\n",
       "      <td>0.639778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>Shia LaBeouf</td>\n",
       "      <td>Megan Fox</td>\n",
       "      <td>0.633615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>Megan Fox</td>\n",
       "      <td>Shia LaBeouf</td>\n",
       "      <td>0.633615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>Mike Myers</td>\n",
       "      <td>Art Carney</td>\n",
       "      <td>0.619520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Art Carney</td>\n",
       "      <td>Mike Myers</td>\n",
       "      <td>0.619520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name           most_similar     score\n",
       "940       Johnny Knoxville            Bonnie Hunt  0.789569\n",
       "781            Bonnie Hunt       Johnny Knoxville  0.789569\n",
       "729          Carrie Fisher            Mark Hamill  0.777845\n",
       "912            Mark Hamill          Carrie Fisher  0.777845\n",
       "902        Martin Lawrence       Johnny Knoxville  0.776004\n",
       "982         Craig Robinson          Danny McBride  0.729525\n",
       "974          Danny McBride         Craig Robinson  0.729525\n",
       "248            Emma Watson       Daniel Radcliffe  0.710853\n",
       "419       Daniel Radcliffe            Emma Watson  0.710853\n",
       "473        William Shatner        Patrick Stewart  0.685567\n",
       "921        Patrick Stewart        William Shatner  0.685567\n",
       "964             Bill Hader            Bonnie Hunt  0.671170\n",
       "783           Mindy Kaling             Bill Hader  0.667186\n",
       "658  Jean-Claude Van Damme           Chuck Norris  0.663727\n",
       "657           Chuck Norris  Jean-Claude Van Damme  0.663727\n",
       "496            June Squibb            Bonnie Hunt  0.639778\n",
       "458           Shia LaBeouf              Megan Fox  0.633615\n",
       "335              Megan Fox           Shia LaBeouf  0.633615\n",
       "821             Mike Myers             Art Carney  0.619520\n",
       "265             Art Carney             Mike Myers  0.619520"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sim = pd.DataFrame(most_similar, columns=[\"name\",\"most_similar\",\"score\"]).sort_values(\"score\", ascending=False)\n",
    "df_sim.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
